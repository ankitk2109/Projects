{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter-rater reliability bwtween rater 1 and rater 2 is: 0.5\n",
      "Inter-rater reliability bwtween rater 1 and rater 3 is: 0.0\n",
      "Inter-rater reliability bwtween rater 2 and rater 3 is: -0.5\n"
     ]
    }
   ],
   "source": [
    "#Inter-rater reliability: It is the degree of agreement among raters. It gives a score of how much homogeneity, \n",
    "#or consensus, there is in the ratings given by judges\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "op1 = \"Audio from this phone is merely OK and this seems to be a side-effect of having basically no room for speakers\"\n",
    "op2 = \"The S Pen is the true headline feature for the Note series and it is pretty much the only flagship around that lets you draw and take notes with a pen on the display.\"\n",
    "op3 = \"Samsung has finally ditched the headphone jack from the Note’s design, meaning you’ll have to rely on wireless headphones or a pair with a USB-C connection\"\n",
    "r1 = [-1,0,-1]\n",
    "r2 = [1,0,-1]\n",
    "r3 = [-1,1,0]\n",
    "print(\"Inter-rater reliability bwtween rater 1 and rater 2 is:\",cohen_kappa_score(r1, r2))\n",
    "print(\"Inter-rater reliability bwtween rater 1 and rater 3 is:\",cohen_kappa_score(r1, r3))\n",
    "print(\"Inter-rater reliability bwtween rater 2 and rater 3 is:\",cohen_kappa_score(r2, r3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons correlation between rater 1 and rater 2 is: 0.0\n",
      "Pearsons correlation between rater 1 and rater 3 is: 0.8660254037844386\n",
      "Pearsons correlation between rater 2 and rater 3 is: -0.4999999999999999\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "r1 = [-1,0,-1]\n",
    "r2 = [1,0,-1]\n",
    "r3 = [-1,1,0]\n",
    "print(\"Pearsons correlation between rater 1 and rater 2 is:\",pearsonr(r1, r2)[0])\n",
    "print(\"Pearsons correlation between rater 1 and rater 3 is:\",pearsonr(r1, r3)[0])\n",
    "print(\"Pearsons correlation between rater 2 and rater 3 is:\",pearsonr(r2, r3)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, math, collections, itertools, sys, os\n",
    "import nltk, nltk.classify.util, nltk.metrics\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics import BigramAssocMeasures, scores\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all words as features\n",
      "train on 7998 instances, test on 2666 instances\n",
      "accuracy: 0.77344336084021\n",
      "pos precision: 0.7881422924901186\n",
      "pos recall: 0.7479369842460615\n",
      "neg precision: 0.7601713062098501\n",
      "neg recall: 0.7989497374343586\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     17.0 : 1.0\n",
      "                   quiet = True              pos : neg    =     15.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "                portrait = True              pos : neg    =     12.4 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "                   flaws = True              pos : neg    =     12.3 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_features(feature_select):\n",
    "    #reading pre-labeled input and splitting into lines\n",
    "    negSentences = open(r\".\\rt-polarity-neg.txt\", 'r', encoding='utf8')\n",
    "    posSentences = open(r\".\\rt-polarity-pos.txt\", 'r', encoding='utf8')\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "    \n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "\n",
    "    # breaks up the sentences into lists of individual words\n",
    "    # creates instance structures for classifier\n",
    "    for i in posSentences:\n",
    "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        posWords = [feature_select(posWords), 'pos']\n",
    "        posFeatures.append(posWords)\n",
    "    #print(posFeatures)\n",
    "    \n",
    "    for i in negSentences:\n",
    "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        negWords = [feature_select(negWords), 'neg']\n",
    "        negFeatures.append(negWords)\n",
    "    #print(negFeatures)\n",
    "\n",
    "    posCutoff = int(math.floor(len(posFeatures)*3/4))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*3/4))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "    #print(\"posFeatures:\",len(posFeatures))\n",
    "    #print(\"negFeatures:\",len(negFeatures))\n",
    "    #print('posCutoff:',posCutoff)\n",
    "    #print('negCutoff:',negCutoff)\n",
    "    #print(trainFeatures)\n",
    "    \n",
    "    #Runs the classifier on the testFeatures\n",
    "    classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "\n",
    "    #Sets up labels to look at output\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\n",
    "    #print(testFeatures[1]) #Output: [{'directed': True, 'with': True, 'purpose': True, 'and': True, 'finesse': True, 'by': True, \"england's\": True, 'roger': True, 'mitchell': True, ',': True, 'who': True, 'handily': True, 'makes': True, 'the': True, 'move': True, 'from': True, 'pleasing': True, 'relatively': True, 'lightweight': True, 'commercial': True, 'fare': True, 'such': True, 'as': True, 'notting': True, 'hill': True, 'to': True, 'real': True, 'thematic': True, 'heft': True, '.': True}, 'pos']\n",
    "    \n",
    "    for i, (features, label) in enumerate(testFeatures): # enumerate adds number-count to each item\n",
    "        #print(\"Labels:\",label)\n",
    "        #print(\"Features\",features)\n",
    "        referenceSets[label].add(i) #create two dictionary of original data called as, referenceSet['pos']={0,2,3,5.....n} and referenceSet['neg']={1,4,6....n}, the dict values are nothing but a unique identifier for each featutre(tokenized sentence) that are being assigned to that dict.\n",
    "        #print('referenceSet=',referenceSets[label])\n",
    "        predicted = classifier.classify(features) # classifiers' proposed polarity for tests\n",
    "        #print(predicted)\n",
    "        testSets[predicted].add(i)\n",
    "        #print(testSets[predicted])\n",
    "\n",
    "    #Outputs\n",
    "    print('train on %s instances, test on %s instances'% (len(trainFeatures), len(testFeatures)))\n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testFeatures))\n",
    "    print('pos precision:', scores.precision(referenceSets['pos'], testSets['pos']))\n",
    "    print('pos recall:', scores.recall(referenceSets['pos'], testSets['pos']))\n",
    "    print('neg precision:', scores.precision(referenceSets['neg'], testSets['neg']))\n",
    "    print('neg recall:', scores.recall(referenceSets['neg'], testSets['neg']))\n",
    "    classifier.show_most_informative_features(10)\n",
    "\n",
    "\n",
    "def make_full_dict(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "print('using all words as features')\n",
    "evaluate_features(make_full_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words and Normalizing(lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all words as features\n",
      "train on 7998 instances, test on 2666 instances\n",
      "accuracy: 0.7678169542385597\n",
      "pos precision: 0.7721036585365854\n",
      "pos recall: 0.759939984996249\n",
      "neg precision: 0.7636632200886263\n",
      "neg recall: 0.7756939234808702\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     17.0 : 1.0\n",
      "                   quiet = True              pos : neg    =     15.7 : 1.0\n",
      "                mediocre = True              neg : pos    =     13.7 : 1.0\n",
      "               absorbing = True              pos : neg    =     13.0 : 1.0\n",
      "                portrait = True              pos : neg    =     12.4 : 1.0\n",
      "              refreshing = True              pos : neg    =     12.3 : 1.0\n",
      "               inventive = True              pos : neg    =     12.3 : 1.0\n",
      "                   flaws = True              pos : neg    =     12.3 : 1.0\n",
      "            refreshingly = True              pos : neg    =     11.7 : 1.0\n",
      "                 triumph = True              pos : neg    =     11.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def evaluate_features(feature_select):\n",
    "    #reading pre-labeled input and splitting into lines\n",
    "    negSentences = open(r\".\\rt-polarity-neg.txt\", 'r', encoding='utf8')\n",
    "    posSentences = open(r\".\\rt-polarity-pos.txt\", 'r', encoding='utf8')\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    # breaks up the sentences into lists of individual words\n",
    "    # creates instance structures for classifier\n",
    "    for i in posSentences:\n",
    "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        for w in posWords: #removing Stop Words\n",
    "            if w in stop_words:\n",
    "                posWords.remove(w)\n",
    "        posWords = [w.lower() for w in posWords]\n",
    "        posWords = [feature_select(posWords), 'pos']\n",
    "        posFeatures.append(posWords)   \n",
    "    for i in negSentences:\n",
    "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        for w in negWords: #removing Stop Words\n",
    "            if w in stop_words:\n",
    "                negWords.remove(w)\n",
    "        negWords = [w.lower() for w in negWords]\n",
    "        negWords = [feature_select(negWords), 'neg']\n",
    "        negFeatures.append(negWords)\n",
    "    posCutoff = int(math.floor(len(posFeatures)*3/4))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*3/4))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "    \n",
    "    #Runs the classifier on the testFeatures\n",
    "    classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "\n",
    "    #Sets up labels to look at output\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\n",
    "    \n",
    "    for i, (features, label) in enumerate(testFeatures): # enumerate adds number-count to each item\n",
    "        referenceSets[label].add(i) \n",
    "        predicted = classifier.classify(features) # classifiers' proposed polarity for tests\n",
    "        testSets[predicted].add(i)\n",
    "\n",
    "    #Outputs\n",
    "    print('train on %s instances, test on %s instances'% (len(trainFeatures), len(testFeatures)))\n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testFeatures))\n",
    "    print('pos precision:', scores.precision(referenceSets['pos'], testSets['pos']))\n",
    "    print('pos recall:', scores.recall(referenceSets['pos'], testSets['pos']))\n",
    "    print('neg precision:', scores.precision(referenceSets['neg'], testSets['neg']))\n",
    "    print('neg recall:', scores.recall(referenceSets['neg'], testSets['neg']))\n",
    "    classifier.show_most_informative_features(10)\n",
    "\n",
    "\n",
    "def make_full_dict(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "print('using all words as features')\n",
    "evaluate_features(make_full_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing classifier to SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all words as features\n",
      "train on 7998 instances, test on 2666 instances\n",
      "accuracy: 0.7475618904726181\n",
      "pos precision: 0.7511415525114156\n",
      "pos recall: 0.7404351087771943\n",
      "neg precision: 0.7440828402366864\n",
      "neg recall: 0.754688672168042\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "def evaluate_features(feature_select):\n",
    "    #reading pre-labeled input and splitting into lines\n",
    "    negSentences = open(r\".\\rt-polarity-neg.txt\", 'r', encoding='utf8')\n",
    "    posSentences = open(r\".\\rt-polarity-pos.txt\", 'r', encoding='utf8')\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "    \n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # breaks up the sentences into lists of individual words\n",
    "    # creates instance structures for classifier\n",
    "    for i in posSentences:\n",
    "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        '''\n",
    "        for w in posWords:\n",
    "            if w in stop_words:\n",
    "                posWords.remove(w)\n",
    "        posWords = [w.lower() for w in posWords]\n",
    "        '''\n",
    "        posWords = [feature_select(posWords), 'pos']\n",
    "        posFeatures.append(posWords)\n",
    "    \n",
    "    for i in negSentences:\n",
    "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        '''\n",
    "        for w in negWords:\n",
    "            if w in stop_words:\n",
    "                negWords.remove(w)\n",
    "        negWords = [w.lower() for w in negWords]\n",
    "        '''\n",
    "        negWords = [feature_select(negWords), 'neg']\n",
    "        negFeatures.append(negWords)\n",
    "\n",
    "    posCutoff = int(math.floor(len(posFeatures)*3/4))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*3/4))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "    \n",
    "    #Runs the classifier on the testFeatures\n",
    "    classifier = SklearnClassifier(LinearSVC()).train(trainFeatures)\n",
    "\n",
    "    #Sets up labels to look at output\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\n",
    "    \n",
    "    \n",
    "    for i, (features, label) in enumerate(testFeatures): # enumerate adds number-count to each item\n",
    "        referenceSets[label].add(i) \n",
    "        predicted = classifier.classify(features) # classifiers' proposed polarity for tests\n",
    "        testSets[predicted].add(i)\n",
    "\n",
    "    #Outputs\n",
    "    print('train on %s instances, test on %s instances'% (len(trainFeatures), len(testFeatures)))\n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testFeatures))\n",
    "    print('pos precision:', scores.precision(referenceSets['pos'], testSets['pos']))\n",
    "    print('pos recall:', scores.recall(referenceSets['pos'], testSets['pos']))\n",
    "    print('neg precision:', scores.precision(referenceSets['neg'], testSets['neg']))\n",
    "    print('neg recall:', scores.recall(referenceSets['neg'], testSets['neg']))\n",
    "    #classifier.show_most_informative_features(10)\n",
    "\n",
    "\n",
    "def make_full_dict(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "print('using all words as features')\n",
    "evaluate_features(make_full_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using all words as features\n",
      "train on 9596 instances, test on 1068 instances\n",
      "accuracy: 0.7902621722846442\n",
      "pos precision: 0.7969348659003831\n",
      "pos recall: 0.7790262172284644\n",
      "neg precision: 0.7838827838827839\n",
      "neg recall: 0.8014981273408239\n",
      "Most Informative Features\n",
      "              engrossing = True              pos : neg    =     20.3 : 1.0\n",
      "                mediocre = True              neg : pos    =     15.7 : 1.0\n",
      "                 generic = True              neg : pos    =     15.0 : 1.0\n",
      "              refreshing = True              pos : neg    =     13.7 : 1.0\n",
      "                 routine = True              neg : pos    =     13.7 : 1.0\n",
      "                  boring = True              neg : pos    =     13.3 : 1.0\n",
      "               inventive = True              pos : neg    =     13.0 : 1.0\n",
      "              disturbing = True              pos : neg    =     13.0 : 1.0\n",
      "            refreshingly = True              pos : neg    =     12.3 : 1.0\n",
      "                    dull = True              neg : pos    =     12.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def evaluate_features(feature_select):\n",
    "    #reading pre-labeled input and splitting into lines\n",
    "    negSentences = open(r\".\\rt-polarity-neg.txt\", 'r', encoding='utf8')\n",
    "    posSentences = open(r\".\\rt-polarity-pos.txt\", 'r', encoding='utf8')\n",
    "    negSentences = re.split(r'\\n', negSentences.read())\n",
    "    posSentences = re.split(r'\\n', posSentences.read())\n",
    "    \n",
    "    posFeatures = []\n",
    "    negFeatures = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # breaks up the sentences into lists of individual words\n",
    "    # creates instance structures for classifier\n",
    "    for i in posSentences:\n",
    "        posWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        '''\n",
    "        for w in posWords:\n",
    "            if w in stop_words:\n",
    "                posWords.remove(w)\n",
    "        posWords = [w.lower() for w in posWords]\n",
    "        '''\n",
    "        posWords = [feature_select(posWords), 'pos']\n",
    "        posFeatures.append(posWords)\n",
    "    \n",
    "    for i in negSentences:\n",
    "        negWords = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        '''\n",
    "        for w in negWords:\n",
    "            if w in stop_words:\n",
    "                negWords.remove(w)\n",
    "        negWords = [w.lower() for w in negWords]\n",
    "        '''\n",
    "        negWords = [feature_select(negWords), 'neg']\n",
    "        negFeatures.append(negWords)\n",
    "\n",
    "    posCutoff = int(math.floor(len(posFeatures)*0.90))\n",
    "    negCutoff = int(math.floor(len(negFeatures)*0.90))\n",
    "    trainFeatures = posFeatures[:posCutoff] + negFeatures[:negCutoff]\n",
    "    testFeatures = posFeatures[posCutoff:] + negFeatures[negCutoff:]\n",
    "    \n",
    "    #Runs the classifier on the testFeatures\n",
    "    classifier = NaiveBayesClassifier.train(trainFeatures)\n",
    "\n",
    "    #Sets up labels to look at output\n",
    "    referenceSets = collections.defaultdict(set)\n",
    "    testSets = collections.defaultdict(set)\n",
    "    \n",
    "    \n",
    "    for i, (features, label) in enumerate(testFeatures): # enumerate adds number-count to each item\n",
    "        referenceSets[label].add(i) \n",
    "        predicted = classifier.classify(features) # classifiers' proposed polarity for tests\n",
    "        testSets[predicted].add(i)\n",
    "\n",
    "    #Outputs\n",
    "    print('train on %s instances, test on %s instances'% (len(trainFeatures), len(testFeatures)))\n",
    "    print('accuracy:', nltk.classify.util.accuracy(classifier, testFeatures))\n",
    "    print('pos precision:', scores.precision(referenceSets['pos'], testSets['pos']))\n",
    "    print('pos recall:', scores.recall(referenceSets['pos'], testSets['pos']))\n",
    "    print('neg precision:', scores.precision(referenceSets['neg'], testSets['neg']))\n",
    "    print('neg recall:', scores.recall(referenceSets['neg'], testSets['neg']))\n",
    "    classifier.show_most_informative_features(10)\n",
    "\n",
    "\n",
    "def make_full_dict(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "print('using all words as features')\n",
    "evaluate_features(make_full_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
